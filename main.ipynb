{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8D2mabhtCeh"
      },
      "source": [
        "# Title of Your Group Project\n",
        "\n",
        "This Python notebook serves as a template for your group project on Experiment 1: Two-Armed Bandit Tasks for the course 'Modeling in Cognitive Science'.\n",
        "This is the practical part of the group project where you get to implement the computational modeling workflow. In this part, you are expected to:\n",
        "\n",
        "\n",
        "*   Implement at least two computational models relevant for your hypothesis. *(3 points)*\n",
        "*   Simulate behavior from the two models. *(3 points)*\n",
        "*   Implement a procedure for fitting the models to data. *(4 points)*\n",
        "*   Implement a procedure for parameter recovery. *(5 points)*\n",
        "*   (Implement a procedure for model recovery.) *(optional; 2 bonus points)*\n",
        "*   Implement a model comparison. *(5 points)*.\n",
        "\n",
        "You can gain a total of 20 points for the practical part of the group project.\n",
        "\n",
        "**Note:** *Some of the exercises below (e.g. Model Simulation) rely on code from previous exercises (e.g., Model Implementation). In such cases, you are encouraged to rely on functions implemented for previous exercises. That is, you don't have to produce redundant code.*\n",
        "\n",
        "For more information, you can check the task's GitHub repository:\n",
        "[2 armed bandit task](https://github.com/snamazova/two_armed_bandit_task)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT71FCKkKZdk"
      },
      "source": [
        "## Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "c7j0N0T6KhMg",
        "outputId": "29e2f804-030b-417f-9d56-d433a7f8850a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# experiment constants\n",
        "num_arms = 2\n",
        "probs = [0.8, 0.2]\n",
        "trial_probaility_change = (\n",
        "    50  # number of trials to change the probability of the chosen option\n",
        ")\n",
        "total_trials = 100  # total number of trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "AACYfiaDLJ5U"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(\"2armed_bandit.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_dhIyzRDni"
      },
      "source": [
        "### DataFrame Structure Overview\n",
        "\n",
        "The DataFrame(df) contains multiple columns representing various aspects of a **two-armed bandit experiment**. Below is a breakdown of the key columns:  \n",
        "\n",
        "## 1. General Trial Information  \n",
        "- **`trial_index`**: The index of the current trial in the experiment (note: displaying the score is also considered a trial, as it is a stimulus).  \n",
        "- **`trial_type`**: The type of trial, indicating how the response was made (e.g., via keyboard or mouse click).  \n",
        "- **`time_elapsed`**: The total time elapsed (in milliseconds) since the start of the experiment.  \n",
        "- **`internal_node_id`**: An internal identifier for the trial structure.  \n",
        "\n",
        "## 2. Response and Stimulus Data  \n",
        "- **`stimulus`**: The presented stimulus for the trial (a `<div>` tag containing the value and color of the bandit).  \n",
        "- **`response`**: The participant’s response/input during the trial.  \n",
        "- **`rt`**: The reaction time (in milliseconds) taken to respond.  \n",
        "\n",
        "## 3. Task-Specific Information (Bean Task)    \n",
        "- **`bean_trial_duration`**: The duration of the bean trial.  \n",
        "- **`bean_duration`**: The time the bean stimulus was displayed.  \n",
        "- **`bean_stimulus`**: The presented stimulus for the trial (a `<div>` tag containing the value and color of the bandit).  \n",
        "- **`bean_choices`**: The available choice options in the trial.  \n",
        "- **`bean_correct_key`**: The correct key (button press) for the task.  \n",
        "- **`bean_type`**: The category of the bean stimulus.  \n",
        "- **`bean_text`**: A text representation of the bean (if applicable).  \n",
        "- **`bean_color`**: The color of the bean, which may be relevant to decision-making.  \n",
        "- **`bean_correct`**: A boolean or categorical value indicating whether the response was correct.  \n",
        "  - *Note:* In the bandit task, there is no strictly correct choice, as the task focuses on exploration rather than accuracy.  \n",
        "- **`bean_value`**: The assigned value or reward associated with the bean (bandit).  \n",
        "- **`bean_score`**: The accumulated score in the experiment.  \n",
        "\n",
        "## 4. Additional Processing Data  \n",
        "- **`bean_html_array`**: An HTML representation of the bean stimulus.  \n",
        "- **`bean_values`**: An array containing value-related information for different beans.  \n",
        "- **`bean_time_after_response`**: The time elapsed after the participant’s response.  \n",
        "- **`bean_bandits`**: Information about Bandit 1 and Bandit 2 (their color and value).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNv8oCa4LyIg",
        "outputId": "005f51c5-ac4f-463b-84ab-bf2eb5427566"
      },
      "outputs": [],
      "source": [
        "# Display the DataFrame columns to analyze it\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "R-F_1BzOOgon"
      },
      "outputs": [],
      "source": [
        "# drop the bean_value column otherwise we have 2 identical value columns later\n",
        "df.drop(columns=[\"bean_value\"], inplace=True, axis=1)\n",
        "\n",
        "# Rename columns for consistency\n",
        "df.rename(columns=lambda x: x.replace(\"bean_\", \"\"), inplace=True)\n",
        "df.rename(columns={\"response\": \"action\", \"value\": \"reward\"}, inplace=True)\n",
        "\n",
        "# filter out instructions, by removing any rows from df where the column 'bandits' has NaN\n",
        "df = df.dropna(subset=[\"bandits\"])\n",
        "\n",
        "# select the columns we need\n",
        "df = df[[\"action\", \"reward\", \"values\", \"score\"]]\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df_participant = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_all_metrics(df, window_size=10, title=None):\n",
        "    \"\"\"\n",
        "    Plot all metrics for a given DataFrame in a 2x2 layout.\n",
        "    Uses previously defined plotting functions.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        The DataFrame containing the data to plot\n",
        "    window_size : int\n",
        "        The window size for the moving average score\n",
        "    title : str, optional\n",
        "        The title for the overall figure\n",
        "    \"\"\"\n",
        "    # Determine if we have q_values\n",
        "    has_q_values = \"q_values\" in df.columns\n",
        "\n",
        "    # Create a 2x2 figure layout\n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Plot 1: Reward Distribution (top left)\n",
        "    ax1 = fig.add_subplot(2, 2, 1)\n",
        "    reward_counts = df[\"reward\"].value_counts().sort_index()\n",
        "    ax1.bar(\n",
        "        [\"No Reward (0)\", \"Reward (1)\"],\n",
        "        reward_counts.values,\n",
        "        color=[\"#ff9999\", \"#66b3ff\"],\n",
        "    )\n",
        "    ax1.set_title(\"Distribution of Rewards vs No Rewards\")\n",
        "    ax1.set_ylabel(\"Count\")\n",
        "    ax1.grid(axis=\"y\", alpha=0.3)\n",
        "    for i, v in enumerate(reward_counts.values):\n",
        "        ax1.text(i, v + 1, str(v), ha=\"center\")\n",
        "\n",
        "    # Plot 2: Moving Average Score (top right)\n",
        "    ax2 = fig.add_subplot(2, 2, 2)\n",
        "    moving_avg = []\n",
        "    for i in range(len(df)):\n",
        "        if i < window_size:\n",
        "            # For the first few trials, use all available data\n",
        "            moving_avg.append(df[\"score\"].iloc[i] / (i + 1))\n",
        "        else:\n",
        "            # Calculate the average score gain over the last window_size trials\n",
        "            moving_avg.append(\n",
        "                (df[\"score\"].iloc[i] - df[\"score\"].iloc[i - window_size]) / window_size\n",
        "            )\n",
        "\n",
        "    ax2.plot(range(len(moving_avg)), moving_avg, color=\"#5c9dd5\", linewidth=2)\n",
        "    ax2.axhline(\n",
        "        y=0.5, color=\"r\", linestyle=\"--\", alpha=0.7, label=\"Random Choice (50%)\"\n",
        "    )\n",
        "    ax2.set_title(f\"Moving Average Score (Window Size: {window_size} trials)\")\n",
        "    ax2.set_xlabel(\"Trial Number\")\n",
        "    ax2.set_ylabel(\"Average Points per Trial\")\n",
        "    ax2.grid(alpha=0.3)\n",
        "    ax2.legend()\n",
        "\n",
        "    # Plot 3: Cumulative Reward (bottom left)\n",
        "    ax3 = fig.add_subplot(2, 2, 3)\n",
        "    ax3.plot(range(len(df)), df[\"score\"], color=\"#5c9dd5\", linewidth=2)\n",
        "    ax3.plot(\n",
        "        range(len(df)),\n",
        "        np.arange(len(df)) * 0.5,\n",
        "        \"r--\",\n",
        "        alpha=0.7,\n",
        "        label=\"Expected Random Choice (50%)\",\n",
        "    )\n",
        "    ax3.set_title(\"Cumulative Reward Over Trials\")\n",
        "    ax3.set_xlabel(\"Trial Number\")\n",
        "    ax3.set_ylabel(\"Total Score\")\n",
        "    ax3.grid(alpha=0.3)\n",
        "    ax3.legend()\n",
        "\n",
        "    # Plot 4: Either Q-Values or Choice History (bottom right)\n",
        "    ax4 = fig.add_subplot(2, 2, 4)\n",
        "    if has_q_values:\n",
        "        q_values = np.array(df[\"q_values\"].tolist())\n",
        "        ax4.plot(range(len(df)), q_values[:, 0], \"b-\", label=\"Arm 0 Q-value\")\n",
        "        ax4.plot(range(len(df)), q_values[:, 1], \"g-\", label=\"Arm 1 Q-value\")\n",
        "        ax4.set_title(\"Q-Values Over Trials\")\n",
        "        ax4.set_xlabel(\"Trial Number\")\n",
        "        ax4.set_ylabel(\"Q-Value\")\n",
        "        ax4.grid(alpha=0.3)\n",
        "        ax4.legend()\n",
        "    else:\n",
        "        # If no q_values, show choice history instead\n",
        "        ax4.plot(range(len(df)), df[\"action\"], color=\"#5c9dd5\", linewidth=2)\n",
        "        ax4.set_title(\"Choice History\")\n",
        "        ax4.set_xlabel(\"Trial Number\")\n",
        "        ax4.set_ylabel(\"Choice\")\n",
        "        ax4.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if title:\n",
        "        plt.subplots_adjust(top=0.93)  # Adjust for the suptitle\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# plotting the data\n",
        "plot_all_metrics(df, window_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HceMyA8DtIZ3"
      },
      "source": [
        "## Model Implementation *(3 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Implement and simulate data from two* models that are suitable to test your hypothesis. *(3 points)*\n",
        "\n",
        "<font size=2>*You may implement more than two models if you wish. However, two models are sufficient for this group project.</font>\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def softmax(x, beta=1):\n",
        "    exp_values = np.exp(x * beta)\n",
        "    return exp_values / np.sum(exp_values)\n",
        "\n",
        "\n",
        "def generate_bandit_data():\n",
        "    bandit_data = []\n",
        "    for i in range(total_trials):\n",
        "        if i < trial_probaility_change:\n",
        "            bandit_data.append(\n",
        "                [int(random.random() <= probs[0]), int(random.random() <= probs[1])]\n",
        "            )\n",
        "        else:\n",
        "            bandit_data.append(\n",
        "                [int(random.random() <= probs[1]), int(random.random() <= probs[0])]\n",
        "            )\n",
        "    return bandit_data\n",
        "\n",
        "\n",
        "def get_reward(action, rewards):\n",
        "    return rewards[action]\n",
        "\n",
        "\n",
        "def log_data(df, trial_number, action, reward, values, action_probabilities):\n",
        "    \"\"\"\n",
        "    Log trial data to the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        The DataFrame to log data to\n",
        "    trial_number : int\n",
        "        The current trial number\n",
        "    action : int\n",
        "        The action taken (0 or 1)\n",
        "    reward : float\n",
        "        The reward received\n",
        "    values : list\n",
        "        The true reward values for each arm\n",
        "    action_probabilities : numpy.ndarray\n",
        "        The current Q-action_probabilities for each arm\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        The updated DataFrame\n",
        "    \"\"\"\n",
        "    # Calculate cumulative score\n",
        "    if df.empty:\n",
        "        score = reward\n",
        "    else:\n",
        "        score = df[\"score\"].iloc[-1] + reward\n",
        "\n",
        "    # Create a new row with all the data\n",
        "    new_row = pd.DataFrame(\n",
        "        {\n",
        "            \"action\": [action],\n",
        "            \"reward\": [reward],\n",
        "            \"values\": [values],\n",
        "            \"score\": [score],\n",
        "            \"action_probabilities\": [action_probabilities]\n",
        "        }\n",
        "    )\n",
        "    # Append the new row to the DataFrame\n",
        "    df = pd.concat([df, new_row], ignore_index=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "isMmbQsKwZ_z"
      },
      "outputs": [],
      "source": [
        "# base abstract model class\n",
        "class AbstractModel:\n",
        "    def __init__(self, params={}):\n",
        "        self.learning_rate = params.get(\"learning_rate\", 0.1)\n",
        "        self.beta = params.get(\"beta\", 1.0)\n",
        "        self.q_values = np.zeros(num_arms)\n",
        "\n",
        "    def choose_action(self):\n",
        "        # using the softmax function to choose the action\n",
        "        probabilities = softmax(self.q_values, self.beta)\n",
        "        return np.random.choice(range(len(probabilities)), p=probabilities)\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        pass\n",
        "\n",
        "    def get_q_values(self):\n",
        "        return self.q_values\n",
        "\n",
        "    def get_action_probability(self, action):\n",
        "        return softmax(self.q_values, self.beta)[action]\n",
        "    \n",
        "    def get_action_probabilities(self):\n",
        "        return softmax(self.q_values, self.beta)\n",
        "    \n",
        "    def step(self, rewards):\n",
        "        action = self.choose_action()\n",
        "        self.update(action, rewards[action])\n",
        "        return action, self.get_action_probabilities()\n",
        "\n",
        "\n",
        "# basic model\n",
        "# only updates the option that was chosen\n",
        "class BasicModel(AbstractModel):\n",
        "    def update(self, action, reward):\n",
        "        # updating the q_values using the formula Qt(a) = Qt−1(a) + α · (rt − Qt−1(a))\n",
        "        self.q_values[action] = self.q_values[action] + self.learning_rate * (\n",
        "            reward - self.q_values[action]\n",
        "        )\n",
        "\n",
        "\n",
        "# Second more complex model\n",
        "# This model also updates the option that was not chosen\n",
        "class ComplexModel(AbstractModel):\n",
        "    def __init__(self, params={}):\n",
        "        super().__init__(params)\n",
        "        self.curiosity = params.get(\"curiosity\", 0.1)\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # updating the q_values using the formula Qt(a) = Qt−1(a) + α · (rt − Qt−1(a))\n",
        "        self.q_values[action] = self.q_values[action] + self.learning_rate * (\n",
        "            reward - self.q_values[action]\n",
        "        )\n",
        "        self.q_values[1 - action] = self.q_values[1 - action] + self.curiosity * (\n",
        "            1 - reward - self.q_values[1 - action]\n",
        "        )\n",
        "\n",
        "\n",
        "# Third model, based on the simple rule:\n",
        "# choose the winning options unitl it lost 2 times then switch to the other option\n",
        "class RuleBasedModel:\n",
        "    def __init__(self, params={}):\n",
        "        self.losses = 0 # starting number of losses\n",
        "        self.current_choice = random.choice([0, 1]) # starting choice\n",
        "        self.loss_threshold = params.get(\"loss_threshold\", 2) # number of losses before switching to the other option\n",
        "\n",
        "    def choose_action(self):\n",
        "        # if the model has lost 2 times, it will switch to the other option\n",
        "        if self.losses >= self.loss_threshold:\n",
        "            self.losses = 0\n",
        "            self.current_choice = 1 - self.current_choice\n",
        "            return self.current_choice\n",
        "        # otherwise stay with the current choice\n",
        "        else:\n",
        "            return self.current_choice\n",
        "\n",
        "    # action is actually not needed for the update here\n",
        "    # but it is useful to have the same interface for all class on update\n",
        "    def update(self, action, reward):\n",
        "        self.losses += 1 - reward\n",
        "\n",
        "    # the action probability is 1 if the action is the one that was chosen and 0 otherwise\n",
        "    def get_action_probability(self, action):\n",
        "        return 1 if self.choose_action() == action else 0\n",
        "    \n",
        "    def get_action_probabilities(self):\n",
        "        return [self.get_action_probability(0), self.get_action_probability(1)]\n",
        "    \n",
        "    def step(self, rewards):\n",
        "        action = self.choose_action()\n",
        "        self.update(action, rewards[action])\n",
        "        return action, self.get_action_probabilities()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twTHrAfL2PC_"
      },
      "source": [
        "## Model Simulation *(3 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Simulate data from both models for a single set of parameters. The simulation should mimic the experiment you are trying to model. *(2 points)*\n",
        "\n",
        "*   Plot the simulated behavior of both models. The model plots should depict key features of the behavioral data, convincing yourself that the models are suitable to capture the data. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_model(model_class, params, data):\n",
        "    # init model and logging df\n",
        "    model = model_class(params)\n",
        "    df_logging = pd.DataFrame(columns=[\"action\", \"reward\", \"values\", \"score\", \"action_probabilities\"])\n",
        "    \n",
        "    for trial in range(total_trials):\n",
        "        action, action_probabilities = model.step(data[trial])\n",
        "        reward = get_reward(action, data[trial])\n",
        "        df_logging = log_data(df_logging, trial, action, reward, data[trial], action_probabilities)\n",
        "        \n",
        "    return df_logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"learning_rate\": 0.1, # updating rate of the q_values\n",
        "    \"beta\": 3, # parameter for the softmax function\n",
        "    \"curiosity\": 0.2, # parameter for updating the q_values of the non chosen option\n",
        "    \"loss_threshold\": 2, # number of losses before switching to the other option for RuleBasedModel\n",
        "}\n",
        "\n",
        "# generate surrogate data\n",
        "bandit_data = generate_bandit_data()\n",
        "\n",
        "# simulate all models\n",
        "df_basic = simulate_model(BasicModel, params, bandit_data)\n",
        "df_complex = simulate_model(ComplexModel, params, bandit_data)\n",
        "df_rule_based = simulate_model(RuleBasedModel, params, bandit_data)\n",
        "\n",
        "df_basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the data of the basic model\n",
        "plot_all_metrics(df_basic, window_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the data of the complex model\n",
        "plot_all_metrics(df_complex, window_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rule based model\n",
        "plot_all_metrics(df_rule_based, window_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VxwTW9LwnvJ"
      },
      "source": [
        "## Parameter Fitting *(4 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Set up a suitable parameter search space *(1 point)*\n",
        "\n",
        "*   Implement a procedure to evaluate the fit of a model based on data *(2 points)*\n",
        "\n",
        "*   Implement a procedure for searching the parameter space. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "K5OKVYszx7tQ"
      },
      "outputs": [],
      "source": [
        "# function to calculate the log likelihood of a model given the data\n",
        "def log_likelihood(model, df):\n",
        "    ll_sum = 0\n",
        "\n",
        "    # repeat the simulation for each trial\n",
        "    for i, row in df.iterrows():\n",
        "        observed_action = row[\"action\"]\n",
        "        observed_reward = row[\"reward\"]        \n",
        "            \n",
        "        # get action probability\n",
        "        action_probability = model.get_action_probability(observed_action)     \n",
        "        \n",
        "        # update the model as if the corection option was chosen\n",
        "        # TODO check if this is the correct way\n",
        "        model.update(observed_action, observed_reward)\n",
        "\n",
        "        # if the action probability is 0, set it to a small number to avoid log(0)\n",
        "        if action_probability <= 0:\n",
        "            action_probability = 1e-10\n",
        "\n",
        "        # update the log likelihood\n",
        "        ll_sum += np.log(action_probability)\n",
        "    return ll_sum\n",
        "\n",
        "\n",
        "def grid_search(model_class, data, param_space):\n",
        "    \"\"\"\n",
        "    Perform grid search to find the best parameters for a model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_class : class\n",
        "        The model class to instantiate (not an instance)\n",
        "    data : list of dict\n",
        "        The data to fit the model to\n",
        "    param_space : dict\n",
        "        Dictionary mapping parameter names to lists of values to search\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    best_params : dict\n",
        "        Dictionary of best parameters\n",
        "    best_ll : float\n",
        "        Log likelihood of the best model\n",
        "    \"\"\"\n",
        "    best_ll = -np.inf\n",
        "    best_params = {}\n",
        "    param_names = list(param_space.keys())\n",
        "\n",
        "    # Generate all parameter combinations\n",
        "    param_combinations = []\n",
        "\n",
        "    # TODO improve to only do nesscary parameter due to the exponentail natrue of grid search,\n",
        "    # it takes a lot of time\n",
        "        \n",
        "    # This function recursivly generates all possible combinations of parameters\n",
        "    def generate_combinations(current_combo, param_idx):\n",
        "        # Base case: if we've reached the end of the parameter names list\n",
        "        if param_idx == len(param_names):\n",
        "            # Append a copy of the current combination to the list of combinations\n",
        "            param_combinations.append(current_combo.copy())\n",
        "            return\n",
        "\n",
        "        # Get the current parameter name\n",
        "        param_name = param_names[param_idx]\n",
        "        # Iterate over all possible values for this parameter\n",
        "        for param_value in param_space[param_name]:\n",
        "            # Set the current parameter value in the current combination\n",
        "            current_combo[param_name] = param_value\n",
        "            # Recursively generate combinations for the next parameter\n",
        "            generate_combinations(current_combo, param_idx + 1)\n",
        "\n",
        "    \n",
        "\n",
        "    generate_combinations({}, 0)\n",
        "\n",
        "    # print(\"parameter combinations\", param_combinations)\n",
        "    \n",
        "    # Evaluate each parameter combination\n",
        "    for params in param_combinations:\n",
        "        model = model_class(params)\n",
        "\n",
        "        # Calculate log likelihood\n",
        "        ll = log_likelihood(model, data)\n",
        "\n",
        "        # Update best parameters if this is better\n",
        "        if ll > best_ll:\n",
        "            best_ll = ll\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_ll\n",
        "\n",
        "# calculate the BIC based on the log likelihood, the number of parameters and the number of samples\n",
        "def calculate_bic(ll, n_params, n_samples):\n",
        "    return np.round(-2 * ll + n_params * np.log(n_samples), 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZgrw_ByFsF"
      },
      "source": [
        "## Parameter Recovery *(5 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Set up a suitable space of parameters relevant for parameter recovery *(1 point)*\n",
        "\n",
        "*   Use the functions above to generate behavior from a models, for a given set of (randomly sampled) parameters, and then fit the model to its generated data. Make sure to evaluate the parameter fit in a quantiative manner. *(3 points)*\n",
        "\n",
        "*   Plot the parameter recovery results for both models. *(1 point)*\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_parameter_recovery(df_recovered, param_space):\n",
        "    \"\"\"\n",
        "    Plot parameter recovery results showing original vs recovered parameters.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_recovered : pandas.DataFrame\n",
        "        DataFrame containing original and recovered parameters\n",
        "    param_space : dict\n",
        "        Dictionary mapping parameter names to their search space\n",
        "    \"\"\"\n",
        "    # Get the number of parameters to plot\n",
        "    n_params = len(param_space.keys())\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(1, n_params, figsize=(5 * n_params, 5))\n",
        "\n",
        "    # If only one parameter, make axes a list for consistent indexing\n",
        "    if n_params == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    # Plot each parameter\n",
        "    for i, param_name in enumerate(param_space.keys()):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Plot the recovered vs original values\n",
        "        ax.scatter(\n",
        "            df_recovered[f\"original_{param_name}\"],\n",
        "            df_recovered[f\"recovered_{param_name}\"],\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        # Plot the ideal recovery line\n",
        "        min_val = min(\n",
        "            df_recovered[f\"original_{param_name}\"].min(),\n",
        "            df_recovered[f\"recovered_{param_name}\"].min(),\n",
        "        )\n",
        "        max_val = max(\n",
        "            df_recovered[f\"original_{param_name}\"].max(),\n",
        "            df_recovered[f\"recovered_{param_name}\"].max(),\n",
        "        )\n",
        "        ax.plot([min_val, max_val], [min_val, max_val], \"r--\", alpha=0.5)\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_xlabel(f\"Original {param_name}\")\n",
        "        ax.set_ylabel(f\"Recovered {param_name}\")\n",
        "        ax.set_title(f\"Parameter Recovery: {param_name}\")\n",
        "        ax.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def test_parameter_recovery(model_class, bandit_data, param_space, num_samples=10):\n",
        "    results = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # choose random parameters for all param_space\n",
        "        params = {}\n",
        "        for param_name in param_space.keys():\n",
        "            params[param_name] = np.random.choice(\n",
        "                param_space[param_name]\n",
        "            )\n",
        "\n",
        "        df_logging = simulate_model(model_class, params, bandit_data)\n",
        "\n",
        "        # fit the models to the data\n",
        "        best_params, best_ll = grid_search(model_class, df_logging, param_space)\n",
        "    \n",
        "        # Create a result dictionary with unpacked parameters\n",
        "        result = {}\n",
        "        for param_name in param_space.keys():\n",
        "            result[f\"original_{param_name}\"] = params[param_name]\n",
        "            result[f\"recovered_{param_name}\"] = best_params[param_name]\n",
        "        results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# correlation between the original and recovered parameters\n",
        "def print_parameter_recovery_correlations(df_recovered, model_name):\n",
        "    \"\"\"\n",
        "    Print a clean correlation matrix focusing on original vs recovered parameters\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_recovered : pandas.DataFrame\n",
        "        DataFrame containing original and recovered parameters\n",
        "    model_name : str\n",
        "        Name of the model for display purposes\n",
        "    \"\"\"\n",
        "    print(f\"\\n{model_name} Parameter Recovery Correlations:\")\n",
        "    \n",
        "    # Get all parameter names (without the 'original_' or 'recovered_' prefix)\n",
        "    param_names = set()\n",
        "    for col in df_recovered.columns:\n",
        "        if col.startswith('original_'):\n",
        "            param_names.add(col.replace('original_', ''))\n",
        "    \n",
        "    # Print correlation between original and recovered for each parameter\n",
        "    print(\"\\nOriginal vs Recovered:\")\n",
        "    for param in param_names:\n",
        "        corr = df_recovered[f'original_{param}'].corr(df_recovered[f'recovered_{param}'])\n",
        "        print(f\"  {param}: {corr:.3f}\")\n",
        "    \n",
        "    # Print correlations between original parameters\n",
        "    if len(param_names) > 1:\n",
        "        print(\"\\nCorrelations between original parameters:\")\n",
        "        orig_cols = [f'original_{param}' for param in param_names]\n",
        "        orig_corr = df_recovered[orig_cols].corr()\n",
        "        for i, param1 in enumerate(param_names):\n",
        "            for param2 in list(param_names)[i+1:]:\n",
        "                corr = orig_corr.loc[f'original_{param1}', f'original_{param2}']\n",
        "                print(f\"  {param1} vs {param2}: {corr:.3f}\")\n",
        "    \n",
        "    # Print correlations between recovered parameters\n",
        "    if len(param_names) > 1:\n",
        "        print(\"\\nCorrelations between recovered parameters:\")\n",
        "        recov_cols = [f'recovered_{param}' for param in param_names]\n",
        "        recov_corr = df_recovered[recov_cols].corr()\n",
        "        for i, param1 in enumerate(param_names):\n",
        "            for param2 in list(param_names)[i+1:]:\n",
        "                corr = recov_corr.loc[f'recovered_{param1}', f'recovered_{param2}']\n",
        "                print(f\"  {param1} vs {param2}: {corr:.3f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "\n",
        "def create_param_space(range, num):\n",
        "    return np.round(np.linspace(range[0], range[1], num), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLdarAD8yXwN"
      },
      "outputs": [],
      "source": [
        "# set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "num_samples = 10 # number of samples to test parameter recovery\n",
        "param_samples = 3 # number of generate params for a parameter space\n",
        "\n",
        "param_space = {\n",
        "    \"learning_rate\": create_param_space([0, 1], param_samples),\n",
        "    \"beta\": create_param_space([0.5, 5], param_samples),\n",
        "    \"curiosity\": create_param_space([0, 1], param_samples),\n",
        "    \"loss_threshold\": create_param_space([1,10], param_samples)\n",
        "}\n",
        "\n",
        "# generate surrogate data for the parameter recovery\n",
        "bandit_data = generate_bandit_data()\n",
        "\n",
        "\n",
        "### POSSIBLE IMPROVEMENTS ###\n",
        "# -  reuse the parameter combiantions of gridsearch across trials and models. (because it takes a lot of time to compute)\n",
        "# - only get the parameter combinations that are needed for the specifc model (no need to compute all combinations)\n",
        "\n",
        "\n",
        "\n",
        "# test the parameter recovery\n",
        "df_recovered_basic = test_parameter_recovery(\n",
        "    BasicModel, bandit_data, param_space, num_samples=num_samples\n",
        ")\n",
        "print(\"basic Model param recovery\", df_recovered_basic)\n",
        "\n",
        "df_recovered_complex = test_parameter_recovery(\n",
        "    ComplexModel, bandit_data, param_space, num_samples=num_samples\n",
        ")\n",
        "print(\"complex Model param recovery\", df_recovered_complex)\n",
        "\n",
        "df_recovered_rule_based = test_parameter_recovery(\n",
        "    RuleBasedModel, bandit_data, param_space, num_samples=num_samples\n",
        ")\n",
        "print(\"rule based Model param recovery\", df_recovered_rule_based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the correlation information\n",
        "print_parameter_recovery_correlations(df_recovered_basic, \"Basic Model\")\n",
        "print_parameter_recovery_correlations(df_recovered_complex, \"Complex Model\")\n",
        "print_parameter_recovery_correlations(df_recovered_rule_based, \"Rule Based Model\")\n",
        "\n",
        "# plot the parameter recovery\n",
        "plot_parameter_recovery(df_recovered_basic, param_space)\n",
        "plot_parameter_recovery(df_recovered_complex, param_space)\n",
        "plot_parameter_recovery(df_recovered_rule_based, param_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naQNlDBfzjnG"
      },
      "source": [
        "## *Optional*: Model Recovery *(2 bonus points)*\n",
        "\n",
        "In this bonus exercise, you may examine model reovery. The bonus points count towards your total group project points. That is, you may accumlate up to 22 points in the practical part of the group project.\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model_recovery(bandit_data, param_space, model_class):     \n",
        "    # choose random parameters for the model\n",
        "    params = {}\n",
        "    for param_name in param_space.keys():\n",
        "        params[param_name] = np.random.choice(param_space[param_name])\n",
        "    \n",
        "    # simulate the data\n",
        "    surrogate_data = simulate_model(model_class, params, bandit_data)\n",
        "\n",
        "    # fit all models to the data\n",
        "    results = {}\n",
        "    _, results[\"BasicModel\"] = grid_search(BasicModel, surrogate_data, param_space)\n",
        "    _, results[\"ComplexModel\"] = grid_search(ComplexModel, surrogate_data, param_space)\n",
        "    _, results[\"RuleBasedModel\"] = grid_search(RuleBasedModel, surrogate_data, param_space)\n",
        "\n",
        "    # return log likelihoods and best parameters for all models\n",
        "    return results\n",
        "\n",
        "def test_model_recovery_multiple(bandit_data, param_space, num_samples=5):\n",
        "    df_results = pd.DataFrame(columns=[\"InputModel\", \"RecoveredModel\"])\n",
        "    \n",
        "    # test the model recovery for all models\n",
        "    model_classes = [BasicModel, ComplexModel, RuleBasedModel]\n",
        "    for model_class in model_classes:\n",
        "        for i in range(num_samples):\n",
        "            # test the model recovery for this model and get log likelihoods and best parameters\n",
        "            results = test_model_recovery(bandit_data, param_space, model_class)  \n",
        "            \n",
        "            print(\"results\", results)         \n",
        "            # get the most likely model, based on the log likelihood of each model\n",
        "            most_likely_model_index = np.argmax([results[\"BasicModel\"], results[\"ComplexModel\"], results[\"RuleBasedModel\"]])\n",
        "            most_likely_model = model_classes[most_likely_model_index]\n",
        "            new_row = pd.DataFrame([[model_class.__name__, most_likely_model.__name__]], columns=df_results.columns)\n",
        "            df_results = pd.concat([df_results, new_row], ignore_index=True)\n",
        "    return df_results\n",
        "\n",
        "# Plot the model recovery results as a heatmap\n",
        "def plot_model_recovery_heatmap(results_df):\n",
        "    # Create a confusion matrix\n",
        "    models = ['BasicModel', 'ComplexModel', 'RuleBasedModel']\n",
        "    confusion_matrix = np.zeros((len(models), len(models)))\n",
        "    \n",
        "    # Count occurrences in each cell\n",
        "    for i, input_model in enumerate(models):\n",
        "        for j, recovered_model in enumerate(models):\n",
        "            count = len(results_df[(results_df['InputModel'] == input_model) & \n",
        "                                  (results_df['RecoveredModel'] == recovered_model)])\n",
        "            confusion_matrix[i, j] = count\n",
        "    \n",
        "    # Normalize by row (input model)\n",
        "    row_sums = confusion_matrix.sum(axis=1, keepdims=True)\n",
        "    confusion_matrix_norm = confusion_matrix / row_sums\n",
        "    \n",
        "    # Create the heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(confusion_matrix_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=models, yticklabels=models)\n",
        "    plt.xlabel('Recovered Model')\n",
        "    plt.ylabel('Input Model')\n",
        "    plt.title('Model Recovery Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print recovery accuracy\n",
        "    diagonal_sum = np.trace(confusion_matrix)\n",
        "    total_samples = confusion_matrix.sum()\n",
        "    print(f\"Overall recovery accuracy: {diagonal_sum}/{total_samples} = {diagonal_sum/total_samples:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIR51ujwziTM"
      },
      "outputs": [],
      "source": [
        "# set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# meta parameters\n",
        "num_samples = 3 # number of samples to test model recovery\n",
        "param_samples = 3 # number of generate params for a parameter space\n",
        "\n",
        "# model parameter spaces\n",
        "param_space = {\n",
        "    \"learning_rate\": create_param_space([0, 1], param_samples),\n",
        "    \"beta\": create_param_space([0.5, 5], param_samples),\n",
        "    \"curiosity\": create_param_space([0, 1], param_samples),\n",
        "    \"loss_threshold\": create_param_space([1,10], param_samples)\n",
        "}\n",
        "\n",
        "# generate surrogate data for the parameter recovery\n",
        "bandit_data = generate_bandit_data()\n",
        "\n",
        "results = test_model_recovery_multiple(bandit_data, param_space, num_samples=num_samples)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_model_recovery_heatmap(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q4dfJ7O0BpW"
      },
      "source": [
        "## Model Comparison *(5 points)*\n",
        "\n",
        "For this exercise you should:\n",
        "\n",
        "*   Load and (potentially) preprocess the experimental data. (1 point)\n",
        "\n",
        "*   Fit the two models to the data.  *(1 point)*\n",
        "\n",
        "*   Evaluate which model performs better, taking into account fit and model complexity. *(2 points)*\n",
        "\n",
        "*   Plot the behavior of the winning model against the data. *(1 point)**\n",
        "\n",
        "Make sure to comment your code and provide an explanation for each code block in a preceding text block.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_participant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wAGI-kd1yRb"
      },
      "outputs": [],
      "source": [
        "# fit the models\n",
        "best_params_basic, best_ll_basic = grid_search(\n",
        "    BasicModel, df_participant, param_space\n",
        ")\n",
        "best_params_complex, best_ll_complex = grid_search(\n",
        "    ComplexModel, df_participant, param_space\n",
        ")\n",
        "best_params_rule_based, best_ll_rule_based = grid_search(\n",
        "    RuleBasedModel, df_participant, param_space\n",
        ")\n",
        "\n",
        "model_classes = [BasicModel, ComplexModel, RuleBasedModel]\n",
        "model_params = [best_params_basic, best_params_complex, best_params_rule_based]\n",
        "model_ll = [best_ll_basic, best_ll_complex, best_ll_rule_based]\n",
        "\n",
        "print(f\"Log Likelihood of Basic Model: {best_ll_basic}\")\n",
        "print(f\"Log Likelihood of Complex Model: {best_ll_complex}\")\n",
        "print(f\"Log Likelihood of Rule-Based Model: {best_ll_rule_based}\")\n",
        "print(\"--------------------------------\")\n",
        "print(f\"BIC of Basic Model: {calculate_bic(best_ll_basic, len(best_params_basic), total_trials)}\")\n",
        "print(f\"BIC of Complex Model: {calculate_bic(best_ll_complex, len(best_params_complex), total_trials)}\")\n",
        "print(f\"BIC of Rule-Based Model: {calculate_bic(best_ll_rule_based, len(best_params_rule_based), total_trials)}\")\n",
        "print(\"--------------------------------\")\n",
        "\n",
        "best_model_class = model_classes[np.argmax(model_ll)]\n",
        "best_model_params = model_params[np.argmax(model_ll)]\n",
        "print(f\"Best model: {best_model_class.__name__}\")\n",
        "\n",
        "# Plot the behavior of the participant\n",
        "print(\"\\nParticipant Data:\")\n",
        "plot_all_metrics(df_participant, window_size=10, title=\"Participant Data\")\n",
        "\n",
        "# Create a new surrogate dataset with the same reward structure as the participant data\n",
        "# Extract the reward structure from participant data\n",
        "participant_env = df_participant['values']\n",
        "\n",
        "# Initialize the best model with the best parameters\n",
        "simulated_data = simulate_model(best_model_class, best_model_params, participant_env)\n",
        "\n",
        "# Plot the behavior of the best model\n",
        "print(f\"\\nBest Model ({best_model_class.__name__}) Simulation:\")\n",
        "plot_all_metrics(simulated_data, window_size=10, title=f\"Best Model ({best_model_class.__name__}) Simulation\")\n",
        "\n",
        "# Compare model performance metrics\n",
        "participant_score = df_participant['score'].iloc[-1]\n",
        "model_score = simulated_data['score'].iloc[-1]\n",
        "\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(f\"Participant final score: {participant_score}\")\n",
        "print(f\"Best model final score: {model_score}\")\n",
        "print(f\"Score difference: {model_score - participant_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After the existing code, add these comparison plots:\n",
        "\n",
        "# Create comparison plots to directly visualize how well the model matches participant behavior\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Plot 1: Compare choices over time\n",
        "axes[0].plot(df_participant['action'], 'b-', label='Participant Choices', alpha=0.7)\n",
        "axes[0].plot(simulated_data['action'], 'r--', label='Model Choices', alpha=0.7)\n",
        "axes[0].set_title('Choice Comparison: Participant vs Model')\n",
        "axes[0].set_xlabel('Trial Number')\n",
        "axes[0].set_ylabel('Choice (0 or 1)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Plot 2: Compare cumulative scores\n",
        "axes[1].plot(df_participant['score'], 'b-', label='Participant Score', alpha=0.7)\n",
        "axes[1].plot(simulated_data['score'], 'r--', label='Model Score', alpha=0.7)\n",
        "axes[1].set_title('Cumulative Score Comparison: Participant vs Model')\n",
        "axes[1].set_xlabel('Trial Number')\n",
        "axes[1].set_ylabel('Cumulative Score')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print choice match percentage\n",
        "choice_matches = sum(df_participant['action'] == simulated_data['action'])\n",
        "match_percentage = (choice_matches / len(df_participant)) * 100\n",
        "print(\"\\nChoice Match Analysis:\")\n",
        "print(f\"Number of matching choices: {choice_matches} out of {len(df_participant)}\")\n",
        "print(f\"Percentage of choices matched: {match_percentage:.2f}%\")\n",
        "\n",
        "# Calculate correlation between participant and model choices\n",
        "# Fix: Convert to numpy arrays first and ensure they're numeric\n",
        "try:\n",
        "    participant_choices = np.array(df_participant['action']).astype(float)\n",
        "    model_choices = np.array(simulated_data['action']).astype(float)\n",
        "    choice_correlation = np.corrcoef(participant_choices, model_choices)[0, 1]\n",
        "    print(f\"Correlation between participant and model choices: {choice_correlation:.3f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not calculate correlation: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
